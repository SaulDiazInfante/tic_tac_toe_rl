{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example usage: TIC-TAC-TOE as a Markov decision process and a solution by Q-Learning\n",
    "\n",
    "## Formulate The Problem As MDP\n",
    "\n",
    "State\n",
    ": Board state of player , which is a 3 x3 grid (- , X , O)\n",
    "\n",
    "Action\n",
    ": Move to an available position.  The player makes a move by indicating the position \n",
    "    $$\n",
    "        [i,j], \\qquad i = 0,1,2;  \\qquad j=0, 1, 2.\n",
    "    $$ \n",
    "The initial condition is random with uniform distribution. \n",
    "Next, we deduce each action according to the  highest Q-value. \n",
    "\n",
    "Transition Probability\n",
    ": \n",
    "\n",
    "Reward\n",
    ": Feedback that agent receives after taking an action being\n",
    "    -  1 if win,\n",
    "    - 0 if loss,\n",
    "    - 0.5 if tie\n",
    "\n",
    "Discount Factor\n",
    ": Determine importance of future reward compared to immediate reward.\n",
    "\n",
    "\n",
    "## Methodology\n",
    "\n",
    "- [] Setup a representation of the tic-tac-toe board and defing the player according to {X, O}\n",
    "- [] Define the $Q$-table, to store the $Q$-values of (sate, action) pairs\n",
    "- [] Perform and monitor a transition according to the $Q$-learning algorithm \n",
    "\n",
    "## Implementing the Q-learning algorithm\n",
    "\n",
    "For each episode do:\n",
    "    \n",
    "    1) Game starts with empty board and a randomly chosen starting player.\n",
    "    \n",
    "    2) Agent choose action based on current state using Epsilon greedy strategy which balance exploration (choosing random action) and exploitation (choose action with high Q-value)\n",
    "    \n",
    "    Q(state,action) = R(state, action)+ learning rate*max(next state, all action)\n",
    "    \n",
    "    3)The agent make move by updating the board based on chosen action.\n",
    "    \n",
    "    5)If the game is over Q-table is updated with final reward based on game outcome.\n",
    "    \n",
    "    If game is not over, Q table is updated based on immediate reward and next state.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem setup and implementation\n",
    "Here we imported necessary libraries. Then created the Tic-Tac-Toe board,  initialize the state of the board and define players. \n",
    "Then define Q-table. The Q-table is used to store the Q-values for each state-action pair in the Tic-Tac-Toe game. The Q-value represents the expected reward when taking a particular action in a specific state."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tic_tac_toe_rl.tic_tac_toe_rl as trl\n",
    "\n",
    "board = np.array(\n",
    "    [\n",
    "        ['-', '-', '-'],\n",
    "        ['-', '-', '-'],\n",
    "        ['-', '-', '-']\n",
    "    ]\n",
    ")\n",
    "players = ['X', 'O']\n",
    "num_players = len(players)\n",
    "Q = {}\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we fix the regarding parameters, and print the board accordingly."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T22:37:41.586443Z",
     "start_time": "2024-08-12T22:37:41.583961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.5\n",
    "num_episodes = 10000\n",
    "\n",
    "par = {\n",
    "    'learning_rate': learning_rate,\n",
    "    'discount_factor': discount_factor,\n",
    "    'exploration_rate': exploration_rate,\n",
    "    'num_episodes': num_episodes\n",
    "}\n",
    "\n",
    "trl.print_board(board)\n",
    "trl.board_to_string(board)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here we config the initial random move."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "empty_cells = np.argwhere(board == '-')\n",
    "action = tuple(random.choice(empty_cells))\n",
    "print(action)\n",
    "agent_wins = 0\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we implement the Q-learning loop"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Main Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    board = np.array(\n",
    "        [\n",
    "            ['-', '-', '-'],\n",
    "            ['-', '-', '-'],\n",
    "            ['-', '-', '-']\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    current_player = random.choice(players)\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "        # Choose an action based on the current state\n",
    "        action = trl.choose_action(board, exploration_rate, Q)\n",
    "        \n",
    "        # Make the chosen move\n",
    "        row, col = action\n",
    "        board[row, col] = current_player\n",
    "        \n",
    "        # Check if the game is over\n",
    "        game_over, winner = trl.is_game_over(board)\n",
    "        \n",
    "        if game_over:\n",
    "            # Update the Q-table with the final reward\n",
    "            if winner == current_player:\n",
    "                reward = 1\n",
    "            elif winner == 'draw':\n",
    "                reward = 0.5\n",
    "            else:\n",
    "                reward = 0\n",
    "            trl.update_q_table(\n",
    "                trl.board_to_string(board),\n",
    "                action,\n",
    "                board,\n",
    "                reward,\n",
    "                Q,\n",
    "                par\n",
    "            )\n",
    "        else:\n",
    "            # Switch to the next player\n",
    "            current_player = players[\n",
    "                (players.index(current_player) + 1) % num_players\n",
    "                ]\n",
    "        \n",
    "        # Update the Q-table based on the immediate reward and the next state\n",
    "        if not game_over:\n",
    "            next_state = trl.board_next_state(action, board, players)\n",
    "            trl.update_q_table(\n",
    "                trl.board_to_string(board),\n",
    "                action,\n",
    "                next_state,\n",
    "            0,\n",
    "                Q,\n",
    "                par\n",
    "            )\n",
    "    \n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Play against the trained agent\n",
    "board = np.array(\n",
    "        [['-', '-', '-'],\n",
    "         ['-', '-', '-'],\n",
    "         ['-', '-', '-']]\n",
    "        )\n",
    "\n",
    "current_player = random.choice(players)\n",
    "game_over = False\n",
    "\n",
    "# ...\n",
    "\n",
    "while not game_over:\n",
    "    if current_player == 'X':\n",
    "        # Human player's turn\n",
    "        trl.print_board(board)\n",
    "        row = int(input(\"Enter the row (0-2): \"))\n",
    "        col = int(input(\"Enter the column (0-2): \"))\n",
    "        action = (row, col)\n",
    "    else:\n",
    "        # Trained agent's turn\n",
    "        action = trl.choose_action(board, 0, Q)\n",
    "    \n",
    "    row, col = action\n",
    "    board[row, col] = current_player\n",
    "    \n",
    "    game_over, winner = trl.is_game_over(board)\n",
    "    \n",
    "    if game_over:\n",
    "        trl.print_board(board)\n",
    "        if winner == 'X':\n",
    "            print(\"Human player wins!\")\n",
    "        elif winner == 'O':\n",
    "            print(\"Agent wins!\")\n",
    "        else:\n",
    "            print(\"It's a draw!\")\n",
    "    else:\n",
    "        current_player = players[\n",
    "            (players.index(current_player) + 1) % num_players]\n",
    "\n",
    "# agent_win_percentage = (agent_wins / num_games) * 100\n",
    "# print(\"Agent win percentage: {:.2f}%\".format(agent_win_percentage))\n",
    "# Main Q-learning algorithm\n",
    "num_draws = 0  # Counter for the number of draws\n",
    "agent_wins = 0  # Counter for the number of wins by the agent\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    board = np.array(\n",
    "            [['-', '-', '-'],\n",
    "             ['-', '-', '-'],\n",
    "             ['-', '-', '-']]\n",
    "            )\n",
    "    \n",
    "    current_player = random.choice(\n",
    "        players\n",
    "        )  # Randomly choose the current player\n",
    "    game_over = False\n",
    "    \n",
    "    while not game_over:\n",
    "        action = trl.choose_action(board, exploration_rate, Q)  # Choose an\n",
    "        # action using the exploration rate\n",
    "        \n",
    "        row, col = action\n",
    "        board[\n",
    "            row, col] = current_player  # Update the board with the current\n",
    "        # player's move\n",
    "        \n",
    "        game_over, winner = trl.is_game_over(board)  # Check if the game is\n",
    "        # over and determine the winner\n",
    "        \n",
    "        if game_over:\n",
    "            if winner == current_player:  # Agent wins\n",
    "                reward = 1\n",
    "                agent_wins += 1\n",
    "            elif winner == 'draw':  # Game ends in a draw\n",
    "                reward = 0\n",
    "                num_draws += 1\n",
    "            else:  # Agent loses\n",
    "                reward = -1\n",
    "            trl.update_q_table(\n",
    "                trl.board_to_string(board),\n",
    "                action,\n",
    "                board,\n",
    "                reward,\n",
    "                Q,\n",
    "                par\n",
    "            )\n",
    "            # Update the Q-table\n",
    "        else:\n",
    "            current_player = players[\n",
    "                (players.index(current_player) + 1) % num_players\n",
    "                ]  # Switch to the next player\n",
    "        if not game_over:\n",
    "            next_state = trl.board_next_state(action, board, players)\n",
    "            trl.update_q_table(\n",
    "                trl.board_to_string(board),\n",
    "                action,\n",
    "                next_state,\n",
    "                0,\n",
    "                Q,\n",
    "                par\n",
    "            )  # Update the Q-table with the next state\n",
    "    exploration_rate *= 0.99  # Decrease the exploration rate over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
